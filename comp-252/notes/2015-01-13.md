Examples of recursive algorithms
================================

Chip testing
    $T_n \leq T_{n/2} = \frac{3n}{2}$

Karatsuba multiplication (aka fractal multiplication)
    $T_n \leq 3 T_{n/2} + 8n$
    Uses the bit model.

Binary search:
    $T_n \leq T_{n/2} + 1$
    Uses an oracle that tells us whether our guess is to the left or the right.

Mergesort:
    $T_n \leq 2 T_{n/2} + n - 1$ if n is even.
    $    \leq T_{\frac{n+1}{2}} + T_{\frac{n-1}{2}} + n - 1$ if n is odd.

Convex hull (CH) in $\R^2$:
    Given n points in the plane, determine the smallest convex shape that
    encloses all the points.
    The northern-, southern-, western-, and eastern-most points are always on
    the convex hull.
    Let's consider the leftmost and rightmost points. We're going to find an
    "upper list" and a "lower list", which are the sections of the convex hull
    joining the leftmost and rightmost points.
    We do it like mergesort. Split up the set of points into two, and find the
    smaller CHs. Then we need to merge the two smaller CHs.
    To perform the merge, imagine a car driving along the list of points to
    merge ordered together by the x-coordinate. You can only make right turns,
    so every time you are forced to make a left turn, kill the previous point.

    This looks like mergesort, so its time complexity is basically the same too.

    $T_n \leq 2 T_{n/2} + 3n$

RECURSIVE PROGRAMS ARE LIKE JUMPING OFF A CLIFF. YOU HAVE TO BELIEVE IT.

Recursion tree
==============

Imaging a recurrence like $T_n = 3 T_{n/4} + n$.

That last n is called the "work", since its the amount of time spent on each node in the recursion tree.

Three important cases:
    1. most time is spent splitting things up.
    2. time is evenly spread out.
    3. most time is spend realizing the obvious solutions at the bottom of the tree.

Master theorem for recurrences
------------------------------

T(n) = a T(n/e) + f(n), if n >= c (constant)
T(n) = 1, if n < c

where a is an integer, e is an integer, f(n) >= 0 (work function)

There are two players: the leaves and the root.

leaves: n ^ log_e a
root: f(n)

Cases:
    1. the leaves win
        n ^ log_e a
        -----------   >= n ^ epsilon
           f(n)
        for epsilon > 0

        Then, T(n) = Theta(n ^ log_e a)

    2. the root wins
           f(n)
        -----------   >= n ^ epsilon
        n ^ log_e a
        for epsilon > 0

        Then, T(n) = Theta( f(n) )

    3. it's even !
        f(n) = Theta( n ^ log_e a )

        Then, T(n) = Theta( f(n) * log(n) )

    4. usually an exercise
        f(n) = n ^ log_e a * log^k n

        Then, T(n) = Theta( f(n) * log^k n )

The theorem applies in the case where we have equality. If equality becomes inequality, then theta becomes big-o.

Matrix multiplication
=====================

C = A * B

Naive matrix multiplication is n^3.

Since c_ij = Sum from k=1 to n of: a_ik b_kj

The Strassen algorithm is faster, but it's not the fastest.

You cut each of the matrices into four, and then by cleverly performing seven
multiplications of the submatrices, you can deduce the overall product. The
time complexity then is Theta( n^2.80... )

http://en.wikipedia.org/wiki/Strassen_algorithm

Euclidean algorithm
===================

The remainders calculated on each step are less than half of the dividend.

Setup: RAM model. T(n) ==def== maximum number of operations of all initial pairs (a, b) with n >= a >= b.

T(n) <= T(n/2) + 2
