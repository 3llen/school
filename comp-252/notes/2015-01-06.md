Preamble
========

My lectures aren't taped, because I like to say things about... colleagues.

Models of computation
=====================

An algorithm is a finite set of instructions that operates on a finite input
and it produces an output which is also finite, and it halts, and it gives you
the correct answer. All of this is essential. It has to be correct. An
algorithm cannot have bugs. An algorithm is by definition correct. It must
halt. So you can't compute all the digits of pi, because there're infinitely
many.

This course's objective is showing how to tackle certain kinds of problems.
We'll deal with big methods for tackling problems: divide and conquer, dynamic
programming, precomputation, etc.

This field needs one thing: HEROES.

Alan Turing suggested a way to think about programs and algorithms: Turing
Machines.

RAM (random access machines), were invented a while later. This is a different
model of computation. The memory contains a finite number of slots, 1...n, each
of which contains data. There is a Central Processing Unit (CPU) with a finite
number of register.

Pointer-based machines consist of an unlimited number of cells, wach of which
has an address that is opaque. Cells can be created or deleted during the
execution of the program.

Time / Complexity
=================

The model of computation is important for discussing the runtime of algorithms.

The RAM model one time unit per *operation*. This isn't physically sensible,
since adding 2 numbers of 100 digits, and 2 numbers of 1 billion digits
shouldn't take the same amount of time.

The bit model considers each bit operation to take one time unit. This is more
in line with how actual processor work, but it's painfult o reason about.

Oracles are devices that answer questions. We consider asking one question to
take one time unit. We can talk about the complexity by considering the number
of times we use the oracle.
