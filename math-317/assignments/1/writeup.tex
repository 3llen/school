\documentclass[11pt,letterpaper]{article}

\author{Jacob Thomas Errington (260636023)}
\title{Assignment \#1\\Numerical analysis -- MATH 317}
\date{Monday, 2 October 2017}

\usepackage[geometry]{jakemath}
\usepackage{listings}

\lstset{
  basicstyle=\footnotesize\ttfamily,
}

\begin{document}

\maketitle

\begin{enumerate}
  \item
    \begin{enumerate}
      \item
        \emph{How accurately do we need to know $\pi$ in order to compute
        $\sqrt{\pi}$ to four decimal places?}

        Generally, if we have a function $f : \R \to \R$ and we want to compute
        the error of $z = f(x)$ given the error $\Delta x$, we use a
        derivative.
        %
        \begin{equation*}
          \Delta z = \Delta x \cdot \deriv{x} f(x)
        \end{equation*}

        Taking $f(x) = \sqrt{x}$, we have
        %
        \begin{equation*}
          \Delta z
          = \Delta x \parens{\frac{1}{2} \cdot \frac{1}{\sqrt{x}}}
        \end{equation*}

        In this problem, we know that $\Delta z \leq 0.00001$, i.e. that we
        want no error in the first four decimal places.
        %
        We substitute and solve to find
        %
        \begin{equation*}
          \Delta x \leq 2 \sqrt{\pi} \cdot 0.00001 \approx 0.000035
        \end{equation*}

      \item
        \emph{Convert the binary number $11.101101$ to base $10$.}

        \begin{equation*}
          (11.101101)_2
          = 2^1 + 2^0 + 2^{-1} + 2^{-3} + 2^{-4} + 2^{-6}
          = \frac{237}{64}
          = 3.703125
        \end{equation*}
    \end{enumerate}

  \item
    \begin{enumerate}
      \item
        \emph{%
          Suppose we want to compute $z = e^x - e^{-x}$ in a small
          neighbourhood of $x$ near zero. What is the most accurate way to do
          this?%
        }

        If we just compute as written, we will have a catastophic cancellation:
        the result will be very close to zero, but the absolute errors on both
        summands will add to produce an error much larger than the result!

        Instead, we can replace the summing of absolute errors with a sum
        of relative errors by multiplying up and down by $e^x$ as follows.
        %
        \begin{equation*}
          z = \frac{e^{2x} - 1}{e^x}
        \end{equation*}

        Let $\Delta x$ be the error on $x$.
        Then the absolute error on $e^x$ is $e^x \Delta x$.
        So the relative error on $e^x$ is
        %
        \begin{equation*}
          \frac{e^x \Delta x)}{e^x} = \Delta x
        \end{equation*}
        %
        Next, the absolute error on $e^{-2x}$ is $2 e^{-2x} \Delta x$,
        and the relative error is hence $2 \Delta x$.

        The relative error on $z$ computed by the division is therefor
        $3 \Delta x$, as we add the relative errors of the divisor and
        dividend.
        Hence, for the absolute error on $z$, we have $\Delta z = 3 z \Delta
        x$. Since $z \approx 0$, this gives an acceptable error on $z$.

      \item
        \emph{%
          How about $z = \bracks{\sqrt{1 + x^2} - \sqrt{1 - x^2}}^{-1}$?%
        }

        We multiply by the conjugate to obtain
        %
        \begin{align*}
          \frac{1}{\sqrt{1 + x^2} - \sqrt{1 - x^2}}
          \cdot
          \frac{\sqrt{1 + x^2} + \sqrt{1 - x^2}}{
            \sqrt{1 + x^2} + \sqrt{1 - x^2}%
          }
          = \frac{\sqrt{1+x^2} + \sqrt{1 - x^2}}{2x^2}
        \end{align*}

        For the denominator, the relative error is $2 \frac{\Delta x}{x}$, i.e.
        twice the relative error of $x$.

        For the numerator, we compute the absolute error using a derivative.
        Let $y = \sqrt{1 + x^2} + \sqrt{1 - x^2}$.
        %
        \begin{align*}
          \Delta y
          &= \deriv[y]{x} \Delta x \\
          &=
          \parens{\frac{x}{\sqrt{1 + x^2}} - \frac{x}{\sqrt{1 - x^2}}}
          \Delta x
        \end{align*}

        We compute the relative error by dividing by $y$.
        %
        \begin{align*}
          \frac{\Delta y}{y}
          &= \frac{
            \parens{\frac{x}{\sqrt{1 + x^2}} - \frac{x}{\sqrt{1 - x^2}}}
          }{
            \sqrt{1 + x^2} + \sqrt{1 - x^2}
          }
          \Delta x
        \end{align*}

        Let's think about this relative error for a moment, keeping in mind
        that $x \approx 0$:
        the denominator is approximately $2$ and numerator is approximately
        zero.
        So the relative error is itself approximately zero here.

        Finally, we compute relative error on $z$ as the sum of the relative
        error of the divisor and of the dividend.
        %
        \begin{equation*}
          \frac{\Delta z}{z}
          =
          2 \Delta x
          +
          \frac{
            \parens{\frac{x}{\sqrt{1 + x^2}} - \frac{x}{\sqrt{1 - x^2}}}
          }{
            \sqrt{1 + x^2} + \sqrt{1 - x^2}
          }
          \Delta x
        \end{equation*}
        %
        but the second term is vanishing, so we can approximate to say
        %
        \begin{equation*}
          \frac{\Delta z}{z}
          \approx 2\Delta x
        \end{equation*}
        %
        so the absolute error on $z$ is $\Delta z = 2 z \Delta x$, which is
        acceptable since $z \approx 0$.
    \end{enumerate}

  \item
    \emph{%
      Compute the root of $f(x) = \sqrt{x} - e^{-x}$ using the secant method
      and the Newton-Raphson method.%
    }

    See appendix \ref{app:q3-results}.

  \item
    \emph{%
      Discuss the behaviour of the fixed point iteration schemes
      $x_{n+1} = e^{-2x_n}$ and
      $x_{n+1} = \frac{1}{2} \ln x_n$.
    }

    For the data, see appendix \ref{app:q4}.

    The exponential scheme appears to converge to the root identified by both
    the secant and Newton-Raphson methods, although considerably more slowly.
    The logarithmic scheme diverges.

    Consider a general iterative scheme $x_{n+1} = \phi(x_n)$.
    We are looking for a fixed point $x^* = \phi(x^*)$.
    Define the error of the $n$\th{} step by $\epsilon_n = x_n - x^*$.
    The Mean Value Theorem tells us that there exists some $c_n$ in the
    interval between $x^*$ and $x^n$ such that
    %
    \begin{equation*}
      \phi^\prime(c_n)
      = \frac{\phi(x_n) - \phi(x^*)}{x_n - x^*}
      = \frac{x_{n+1} - x^*}{x_n - x^*}
      = \frac{\epsilon_{n+1}}{\epsilon_n}
    \end{equation*}
    %
    so $\epsilon_{n+1} = \phi^\prime(c_n) \epsilon_n$.

    What this means for us that if the derivative of our iteration scheme is
    less than $1$ everywhere in our search interval, the errors on successive
    iterations keep getting smaller. Otherwise, they get larger.
    So it suffices to look at the derivatives of the iteration schemes we have
    to understand why one converges and why one diverges.

    First consider $\phi(x) = e^{-2x}$.
    Its derivative is $\phi^\prime(x) = -2 * e^{-2x}$.
    Since we are considering a search interval in the positive reals, then
    indeed this derivative is at most $1$ pretty much everywhere for $x > 0$.
    There's a slight caveat: the derivative does go above $1$ for
    $x \in [0, x_0]$ for some $x_0$ between $0.3$ and $0.4$.
    However, our root is above $0.4$, so we shouldn't ever stray into that
    region of the search space.
    Even so, this scheme appears to converge for an initial guess of $0.25$.

    Next we consider the logarithmic scheme
    $\phi(x) = \frac{1}{2} \ln x$.
    Its derivative is $\phi^\prime(x) = \frac{1}{2x}$.
    This derivative is greater than $1$ at the root, so in essence we get
    ``pushed away'' from the root rather than pulled towards it when we iterate
    this function.

  \item
    Consider the fixed point iteration scheme
    %
    \begin{equation*}
      \phi(x) = \frac{\lambda x + 1 - \sin x}{1 + \lambda}
    \end{equation*}
    %
    We wish to choose $\lambda$ so that this scheme converges as fast as
    possible to the root of the equation $1 - x - \sin x = 0$.

    From the analysis we did in the previous problem involving the Mean Value
    Theorem, we know that the error of the next guess depends on the magnitude
    of the derivative at the current guess. So we want to make this magnitude
    as close to zero as possible.

    First, we compute the derivative.
    %
    \begin{equation*}
      \phi^\prime(x) = \frac{\lambda - \cos x}{1 + \lambda}
    \end{equation*}

    In general, the numerator will be at most $1 + \lambda$ and at least
    $\lambda - 1$.

    Let's look at some cases.
    If we take $\lambda = 0$,
    then $\phi^\prime(x)$ oscillates between $-1$ and $1$.
    If we take $\lambda = 1$,
    then $\phi^\prime(x)$ oscillates between $0$ and $1$.
    If we take $\lambda = 2$,
    then $\phi^\prime(x)$ oscillates between $\frac{2}{3}$ and $1$.

    For even greater values of $\lambda$, the minimum moves further away from
    zero, which is undesirable.

    I think that choosing $\lambda = 1$ will give the fastest convergence: the
    cosine function spends a lot of its time near its extrema and relatively
    quickly passes from $-1$ to $1$.

    From the numerical data in appendix \ref{app:q5}, the root is at
    $x = 0.510973$.
\end{enumerate}

\appendix

\section{Source code}

What follows is the Haskell program used to compute the results.

\lstinputlisting[language=Haskell]{code/a1.hs}

\section{Results for question 3}
\label{app:q3-results}

\subsection{With the secant method}

\lstinputlisting{code/secant-results.txt}

\subsection{With the Newton-Raphson method}

\lstinputlisting{code/nr-results.txt}

\section{Results for question 4}
\label{app:q4}

\subsection{Exponential fixed point}

We use the fixed point scheme $x_{n+1} = e^{-2x_n}$.

\lstinputlisting{code/fix1-results.txt}

\subsection{Logarithmic fixed point}

We use the fixed point scheme $x_{n+1} = \frac{1}{2} \ln x_n$.

\lstinputlisting{code/fix2-results.txt}

\section{Results for question 5}

We use the fixed point scheme
\begin{equation*}
  x_{n+1} = \frac{\lambda x_n + 1 - \sin {x_n}}{1 + \lambda}
\end{equation*}

\lstinputlisting{code/fixl-results.txt}

\end{document}
